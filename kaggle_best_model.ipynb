{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf7a1d1b",
   "metadata": {},
   "source": [
    "# Kaggle Submission Notebook\n",
    "本笔记本基于本地脚本实验结果，复现最优 XGBoost 配置以便在 Kaggle 环境中直接生成 `submission.csv`。\n",
    "\n",
    "- 数据路径可根据实际数据集挂载位置调整。\n",
    "- 所有说明均提供中英文，便于快速对照。\n",
    "- 最终会导出包含 `loan_status` 预测的 `submission.csv` 以便直接提交。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0510132a",
   "metadata": {},
   "source": [
    "## 1. 数据读取与预处理流程 / Data Loading & Preprocessing\n",
    "本节搭建可复用的预处理 pipeline：加载 train/test、填补缺失值、LabelEncoder 编码类别特征、数值特征标准化（含 dtype 兼容处理），以保证 Kaggle Notebook 与本地脚本一致。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a979db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports & Paths / 导入与路径设置 ---\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Kaggle datasets are mounted under /kaggle/input; fall back to local data/ when testing offline.\n",
    "# Kaggle 数据默认位于 /kaggle/input 下，若在本地运行则回退到 data/ 目录。\n",
    "DATA_DIR = Path(\"/kaggle/input/loanpractice\")\n",
    "if not DATA_DIR.exists():\n",
    "    DATA_DIR = Path(\"/kaggle/input/loan-default-prediction\")\n",
    "if not DATA_DIR.exists():\n",
    "    DATA_DIR = Path(\"data\")\n",
    "\n",
    "TRAIN_PATH = DATA_DIR / \"train.csv\"\n",
    "TEST_PATH = DATA_DIR / \"test.csv\"\n",
    "TARGET_COL = \"loan_status\"\n",
    "print(f\"Using data dir: {DATA_DIR}\")\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "print(train_df.shape, test_df.shape)\n",
    "\n",
    "\n",
    "def preprocess_data(train_df: pd.DataFrame, test_df: pd.DataFrame, target_col: str = TARGET_COL):\n",
    "    \"\"\"Replicates项目预处理逻辑：缺失值、编码、缩放，并返回特征/目标。\"\"\"\n",
    "    train = train_df.copy()\n",
    "    test = test_df.copy()\n",
    "\n",
    "    # Identify feature groups / 识别特征类型\n",
    "    exclude_cols = [target_col, \"id\", \"Id\", \"ID\"]\n",
    "    feature_cols = [col for col in train.columns if col not in exclude_cols]\n",
    "    categorical_cols = train[feature_cols].select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "    numerical_cols = train[feature_cols].select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "\n",
    "    # Fill missing values / 填补缺失\n",
    "    for col in numerical_cols:\n",
    "        median_val = train[col].median()\n",
    "        train[col] = train[col].fillna(median_val)\n",
    "        if col in test.columns:\n",
    "            test[col] = test[col].fillna(median_val)\n",
    "    for col in categorical_cols:\n",
    "        mode_val = train[col].mode()[0] if not train[col].mode().empty else \"Unknown\"\n",
    "        train[col] = train[col].fillna(mode_val)\n",
    "        if col in test.columns:\n",
    "            test[col] = test[col].fillna(mode_val)\n",
    "\n",
    "    # Label encode categoricals / LabelEncoder 编码\n",
    "    encoders: dict[str, LabelEncoder] = {}\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        if col in test.columns:\n",
    "            combined = pd.concat([train[col].astype(str), test[col].astype(str)], axis=0)\n",
    "        else:\n",
    "            combined = train[col].astype(str)\n",
    "        le.fit(combined)\n",
    "        train[col] = le.transform(train[col].astype(str))\n",
    "        if col in test.columns:\n",
    "            test[col] = le.transform(test[col].astype(str))\n",
    "        encoders[col] = le\n",
    "\n",
    "    # Align columns available in both train & test / 仅保留同时存在的特征\n",
    "    test_feature_cols = [col for col in feature_cols if col in test.columns]\n",
    "    X_train = train[test_feature_cols].copy()\n",
    "    X_test = test[test_feature_cols].copy()\n",
    "\n",
    "    # Scale numeric columns with float casting / 数值列转 float 再缩放\n",
    "    scaler = StandardScaler()\n",
    "    num_cols_in_features = [col for col in numerical_cols if col in test_feature_cols]\n",
    "    if num_cols_in_features:\n",
    "        X_train.loc[:, num_cols_in_features] = X_train[num_cols_in_features].astype(np.float64)\n",
    "        X_test.loc[:, num_cols_in_features] = X_test[num_cols_in_features].astype(np.float64)\n",
    "        X_train.loc[:, num_cols_in_features] = scaler.fit_transform(X_train[num_cols_in_features])\n",
    "        X_test.loc[:, num_cols_in_features] = scaler.transform(X_test[num_cols_in_features])\n",
    "\n",
    "    y_train = train[target_col].astype(int)\n",
    "    test_ids = test[\"id\"] if \"id\" in test.columns else pd.Series(test.index, name=\"id\")\n",
    "\n",
    "    return X_train, y_train, X_test, test_ids.reset_index(drop=True), test_feature_cols, encoders, scaler\n",
    "\n",
    "\n",
    "X_train, y_train, X_test, test_ids, feature_cols, encoders, scaler = preprocess_data(train_df, test_df)\n",
    "print(f\"Feature count: {len(feature_cols)} | Train samples: {len(X_train)} | Test samples: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06de8be",
   "metadata": {},
   "source": [
    "## 2. 定义候选模型与超参数空间 / Candidate Models & Hyper-Parameters\n",
    "结合本地实验，声明包含 Logistic、RandomForest、GradientBoosting、XGBoost 等候选配置，后续循环可直接实例化评估。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91fa963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "    HAS_LGBM = True\n",
    "except ImportError:\n",
    "    HAS_LGBM = False\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    HAS_XGB = True\n",
    "except ImportError:\n",
    "    HAS_XGB = False\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Pre-defined hyper-parameter spaces / 预设超参数\n",
    "MODEL_SPECS: dict[str, dict] = {\n",
    "    \"logistic\": {\n",
    "        \"description\": \"Logistic Regression baseline\",\n",
    "        \"params\": {\n",
    "            \"max_iter\": 1000,\n",
    "            \"C\": 0.1,\n",
    "            \"solver\": \"lbfgs\",\n",
    "            \"random_state\": RANDOM_STATE,\n",
    "        },\n",
    "        \"builder\": lambda: LogisticRegression(\n",
    "            max_iter=1000, C=0.1, solver=\"lbfgs\", random_state=RANDOM_STATE\n",
    "        ),\n",
    "    },\n",
    "    \"random_forest\": {\n",
    "        \"description\": \"RandomForest depth-15\",\n",
    "        \"params\": {\n",
    "            \"n_estimators\": 200,\n",
    "            \"max_depth\": 15,\n",
    "            \"min_samples_split\": 5,\n",
    "            \"min_samples_leaf\": 2,\n",
    "            \"n_jobs\": -1,\n",
    "            \"random_state\": RANDOM_STATE,\n",
    "        },\n",
    "        \"builder\": lambda: RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=15,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            n_jobs=-1,\n",
    "            random_state=RANDOM_STATE,\n",
    "        ),\n",
    "    },\n",
    "    \"gradient_boosting\": {\n",
    "        \"description\": \"Gradient Boosting tuned\",\n",
    "        \"params\": {\n",
    "            \"n_estimators\": 150,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"max_depth\": 5,\n",
    "            \"min_samples_split\": 5,\n",
    "            \"random_state\": RANDOM_STATE,\n",
    "        },\n",
    "        \"builder\": lambda: GradientBoostingClassifier(\n",
    "            n_estimators=150,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            min_samples_split=5,\n",
    "            random_state=RANDOM_STATE,\n",
    "        ),\n",
    "    },\n",
    "}\n",
    "\n",
    "if HAS_LGBM:\n",
    "    MODEL_SPECS[\"lightgbm\"] = {\n",
    "        \"description\": \"LightGBM tuned\",\n",
    "        \"params\": {\n",
    "            \"n_estimators\": 200,\n",
    "            \"learning_rate\": 0.05,\n",
    "            \"max_depth\": 7,\n",
    "            \"num_leaves\": 31,\n",
    "            \"subsample\": 0.8,\n",
    "            \"colsample_bytree\": 0.8,\n",
    "            \"random_state\": RANDOM_STATE,\n",
    "        },\n",
    "        \"builder\": lambda: LGBMClassifier(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=7,\n",
    "            num_leaves=31,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=RANDOM_STATE,\n",
    "            verbose=-1,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "if HAS_XGB:\n",
    "    xgb_params = {\n",
    "        \"n_estimators\": 200,\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"max_depth\": 7,\n",
    "        \"min_child_weight\": 3,\n",
    "        \"subsample\": 0.8,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"reg_lambda\": 1.0,\n",
    "        \"gamma\": 0.0,\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "        \"eval_metric\": \"logloss\",\n",
    "        \"n_jobs\": -1,\n",
    "    }\n",
    "    MODEL_SPECS[\"xgboost_best\"] = {\n",
    "        \"description\": \"Best single model from experiments\",\n",
    "        \"params\": xgb_params,\n",
    "        \"builder\": lambda params=xgb_params: XGBClassifier(**params),\n",
    "    }\n",
    "\n",
    "MODEL_SPECS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdfbe50",
   "metadata": {},
   "source": [
    "## 3. 交叉验证评估与结果记录 / Cross-Validation & Logging\n",
    "使用 StratifiedKFold 循环训练每个候选模型，计算 ROC-AUC 与 RMSE（$RMSE=\\sqrt{\\frac{1}{n}\\sum (y-\\hat y)^2}$），并汇总为 DataFrame 以便追踪最优配置。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e62573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "cv_records: list[dict] = []\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "for model_key, spec in MODEL_SPECS.items():\n",
    "    builder = spec.get(\"builder\")\n",
    "    if builder is None:\n",
    "        print(f\"Skip {model_key}: builder not available\")\n",
    "        continue\n",
    "\n",
    "    auc_scores: list[float] = []\n",
    "    rmse_scores: list[float] = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train), start=1):\n",
    "        model = builder()\n",
    "        X_tr_fold = X_train.iloc[train_idx]\n",
    "        y_tr_fold = y_train.iloc[train_idx]\n",
    "        X_val_fold = X_train.iloc[val_idx]\n",
    "        y_val_fold = y_train.iloc[val_idx]\n",
    "\n",
    "        model.fit(X_tr_fold, y_tr_fold)\n",
    "        y_prob = model.predict_proba(X_val_fold)[:, 1]\n",
    "        y_pred = (y_prob > 0.5).astype(int)\n",
    "\n",
    "        auc_scores.append(roc_auc_score(y_val_fold, y_prob))\n",
    "        rmse_scores.append(mean_squared_error(y_val_fold, y_prob, squared=False))\n",
    "\n",
    "    record = {\n",
    "        \"model_key\": model_key,\n",
    "        \"description\": spec[\"description\"],\n",
    "        \"roc_auc_mean\": float(np.mean(auc_scores)),\n",
    "        \"roc_auc_std\": float(np.std(auc_scores)),\n",
    "        \"rmse_mean\": float(np.mean(rmse_scores)),\n",
    "        \"rmse_std\": float(np.std(rmse_scores)),\n",
    "        \"params\": spec[\"params\"],\n",
    "    }\n",
    "    cv_records.append(record)\n",
    "\n",
    "cv_results = pd.DataFrame(cv_records).sort_values(\"roc_auc_mean\", ascending=False).reset_index(drop=True)\n",
    "cv_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7944f7",
   "metadata": {},
   "source": [
    "## 4. 选取最优模型并保存配置 / Select Best Model & Persist Config\n",
    "按照 ROC-AUC 均值排序，挑选得分最高的记录，并把关键参数、特征列表与随机种子序列化，便于后续直接复现。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18db002d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_entry = cv_results.iloc[0]\n",
    "best_model_key = best_entry[\"model_key\"]\n",
    "best_model_desc = best_entry[\"description\"]\n",
    "\n",
    "best_config = {\n",
    "    \"model_key\": best_model_key,\n",
    "    \"description\": best_model_desc,\n",
    "    \"params\": MODEL_SPECS[best_model_key][\"params\"],\n",
    "    \"features\": feature_cols,\n",
    "    \"random_state\": RANDOM_STATE,\n",
    "}\n",
    "\n",
    "print(\"Best model by ROC-AUC:\", best_model_key)\n",
    "print(json.dumps(best_config, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60210e4a",
   "metadata": {},
   "source": [
    "## 5. 训练全量模型并生成测试集预测 / Fit Full Model & Score Test\n",
    "根据最优配置重新在全部训练数据上拟合模型，记录耗时与特征重要性，以便在 Kaggle Notebook 中复现本地最佳方案。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b377a623",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_builder = MODEL_SPECS[best_model_key][\"builder\"]\n",
    "best_model = best_builder()\n",
    "\n",
    "start_time = time.time()\n",
    "best_model.fit(X_train, y_train)\n",
    "train_seconds = time.time() - start_time\n",
    "\n",
    "print(f\"Trained {best_model_key} in {train_seconds:.2f}s with {len(feature_cols)} features\")\n",
    "\n",
    "# Feature importance if available / 若支持则输出特征重要性\n",
    "if hasattr(best_model, \"feature_importances_\"):\n",
    "    importance_df = (\n",
    "        pd.DataFrame({\n",
    "            \"feature\": feature_cols,\n",
    "            \"importance\": best_model.feature_importances_,\n",
    "        })\n",
    "        .sort_values(\"importance\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    display(importance_df.head(15))\n",
    "else:\n",
    "    print(\"Model does not expose feature_importances_.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd82037",
   "metadata": {},
   "source": [
    "## 6. 生成 submission.csv 并展示样例 / Create submission.csv & Preview\n",
    "利用全量模型对测试集输出概率与标签，写入 `submission.csv`（含 `id`, `loan_status`, `probability`）。预览文件头部并确认可直接在 Kaggle 提交。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b8debc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "test_pred = (test_prob > 0.5).astype(int)\n",
    "\n",
    "submission = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": test_ids,\n",
    "        \"loan_status\": test_pred,\n",
    "        \"probability\": test_prob,\n",
    "    }\n",
    ")\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"submission.csv saved with shape:\", submission.shape)\n",
    "submission.head()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
